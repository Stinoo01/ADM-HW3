{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "flag = False # run only once\n",
    "\n",
    "if flag:\n",
    "    # URL of the website\n",
    "    url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='  \n",
    "    prefix = '/masters-degrees/course/'\n",
    "    exclude = ['\\nMore details \\n', '\\nRead more \\n', '\\xa0Video(s)', '\\xa0Student Profile(s)'] \n",
    "\n",
    "    # Create a list to store the URLs of the masters\n",
    "    master_urls = []\n",
    "\n",
    "    # Loop through the first 400 pages\n",
    "    for page_number in range(1, 401):\n",
    "        print(url + str(page_number))\n",
    "        response = requests.get(url + str(page_number))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "            # Use BeautifulSoup to extract the URLs and append them to the master_urls list\n",
    "            for link in soup.find_all('a', {'class':'courseLink'}):\n",
    "                if link['href'][:len(prefix)] == prefix and not link.text in exclude:\n",
    "                    master_urls.append((link['href'], link.text))\n",
    "\n",
    "\n",
    "    # Save the collected URLs in a text file\n",
    "    with open(\"master_urls.txt\", \"a\") as file:\n",
    "        for url in master_urls:\n",
    "            file.write(url[0] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "\n",
    "flag = False\n",
    "\n",
    "if flag:\n",
    "    # Open the file and read its content\n",
    "    with open(\"master_urls.txt\", \"r\") as file:\n",
    "        master_urls = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    # Create a directory to store HTML pages\n",
    "    output_root_directory = \"html_pages\"\n",
    "    os.makedirs(output_root_directory, exist_ok=True)  # create the directory if it doen't exist\n",
    "\n",
    "    # Read 15 URLs at a time and create HTML pages\n",
    "    subset_size = 15\n",
    "    for i in range(0, len(master_urls), subset_size):\n",
    "        subset = master_urls[i:i + subset_size] # extract 15 more urls\n",
    "\n",
    "        # Create a subfolder for each page\n",
    "        output_directory = os.path.join(output_root_directory, f\"page_{i // subset_size + 1}\")\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Create an HTML page for each URL in the subset\n",
    "        for url in subset:\n",
    "            prefix = 'https://www.findamasters.com/'\n",
    "            response = requests.get(prefix + url)  # sends a GET request \n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                page_content = soup.prettify()  # extract the content of the page          \n",
    "                master_name = url.split(\"/\")[-2]  # extract the name of the master from the URL \n",
    "\n",
    "                # Check if the file already exists, and append a number if necessary\n",
    "                page_filename = f\"{output_directory}/{master_name}.html\"\n",
    "                counter = 1\n",
    "                while os.path.exists(page_filename):\n",
    "                    page_filename = f\"{output_directory}/{master_name}({counter}).html\"\n",
    "                    counter += 1\n",
    "\n",
    "                # Save the content in a HTML file\n",
    "                with open(page_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(page_content)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        print(f\"page {i // subset_size + 1} completed\")\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
