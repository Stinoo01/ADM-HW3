{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MY IDEA --> read the txt file with the url\n",
    "#with open('master_urls.txt', 'r') as file:\n",
    "    # Read the content of the file\n",
    "#    content = file.read()\n",
    "\n",
    "# Split the content into a list of URLs\n",
    "#url_list = content.split('\\n')\n",
    "\n",
    "# Print the list of URLs\n",
    "#for url in url_list:\n",
    "#    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msc_page_url = \"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\"\n",
    "\n",
    "#msc_page_url = [\"https://www.findamasters.com\" + element if not element.startswith(\"http\") else element for element in url_list]\n",
    "\n",
    "\n",
    "#DEFINE THE FUNCTION TO EXTRACT INFORMATION\n",
    "\n",
    "def extract_msc_page(msc_page_url):\n",
    "  \n",
    "  contents = {'url': \"https://www.findamasters.com\" + msc_page_url}\n",
    "\n",
    "  page_url = requests.get(msc_page_url)\n",
    "  page_soup = BeautifulSoup(page_url.text)\n",
    "\n",
    "\n",
    "  #course name\n",
    "  page_links = page_soup.find('h1', {'class': 'text-white course-header__course-title'})\n",
    "  name = page_links.get_text() \n",
    "  contents[\"courseName\"] = name\n",
    "\n",
    "  #university name\n",
    "  page_links = page_soup.find_all('a', {'class': 'course-header__institution'})\n",
    "  contents['universityName'] = page_links[0].contents[0] \n",
    "\n",
    "  #faculty name\n",
    "  page_links = page_soup.find_all('a', {'class': 'course-header__department'})\n",
    "  contents['facultyName'] = page_links[0].contents[0] \n",
    "\n",
    "  #full time\n",
    "  try:\n",
    "    page_links= page_soup.find('a', {'class':'inheritFont concealLink text-decoration-none text-gray-600'})\n",
    "    time = page_links.get_text()\n",
    "    contents['isItFullTime'] = time\n",
    "  except AttributeError:\n",
    "    contents['isItFullTime'] = \"\"\n",
    "\n",
    "  #description\n",
    "  page_links = page_soup.find('div', {'id': 'Snippet'})\n",
    "  description = page_links.get_text() \n",
    "  contents[\"description\"] = description\n",
    "\n",
    "  #starting date\n",
    "  page_links = page_soup.find('span', {'class': 'key-info__content key-info__start-date py-2 pr-md-3 text-nowrap d-block d-md-inline-block'})\n",
    "  starting = page_links.get_text() \n",
    "  contents[\"startDate\"] = starting\n",
    "\n",
    "  #fees\n",
    "  page_links = page_soup.find('div', {'class': 'course-sections course-sections__fees tight col-xs-24'})\n",
    "  fees = page_links.get_text() \n",
    "  contents[\"fees\"] = fees\n",
    "\n",
    "  #modality\n",
    "  try:\n",
    "    page_links = page_soup.find('a', {'title': 'View all MSc courses'})\n",
    "    modality = page_links.get_text() \n",
    "    contents[\"modality\"] = modality\n",
    "  except AttributeError:\n",
    "    contents['modality'] = \"\"\n",
    "\n",
    "  #Duration\n",
    "  page_links = page_soup.find('span', {'class': 'key-info__content key-info__duration py-2 pr-md-3 d-block d-md-inline-block'})\n",
    "  duration = page_links.get_text() \n",
    "  contents[\"duration\"] = duration\n",
    "\n",
    "  #city\n",
    "  try:\n",
    "    page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__city'})\n",
    "    city = page_links.get_text() \n",
    "    contents[\"city\"] = city\n",
    "  except AttributeError:\n",
    "    contents['modality'] = \"\"\n",
    "\n",
    "  #country\n",
    "  try:\n",
    "    page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__country'})\n",
    "    country = page_links.get_text() \n",
    "    contents[\"country\"] = country\n",
    "  except AttributeError:\n",
    "    contents['modality'] = \"\"\n",
    "\n",
    "  #administration\n",
    "  try:\n",
    "    page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__on-campus'})\n",
    "    administration = page_links.get_text() \n",
    "    contents[\"administration\"] = administration\n",
    "  except AttributeError:\n",
    "        contents[\"administration\"] = \"\"\n",
    "    \n",
    "  return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_msc_page_from_html(html_content):\n",
    "    contents = {}\n",
    "\n",
    "    # Parse HTML content\n",
    "    page_soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # URL\n",
    "    contents['url'] = \"https://www.findamasters.com\"\n",
    "\n",
    "    # Course name\n",
    "    try:\n",
    "        page_links = page_soup.find('h1', {'class': 'text-white course-header__course-title'})\n",
    "        name = page_links.get_text()\n",
    "        contents[\"courseName\"] = name.strip()\n",
    "    except AttributeError:\n",
    "        contents['courseName'] = \"\"\n",
    "\n",
    "    # University name\n",
    "    try:\n",
    "        page_links = page_soup.find_all('a', {'class': 'course-header__institution'})\n",
    "        contents['universityName'] = page_links[0].contents[0].strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        contents['universityName'] = \"\"\n",
    "\n",
    "    # Faculty name\n",
    "    try:\n",
    "        page_links = page_soup.find_all('a', {'class': 'course-header__department'})\n",
    "        contents['facultyName'] = page_links[0].contents[0].strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        contents['facultyName'] = \"\"\n",
    "\n",
    "    # Full time\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'inheritFont concealLink text-decoration-none text-gray-600'})\n",
    "        time = page_links.get_text().strip()\n",
    "        contents['isItFullTime'] = time\n",
    "    except AttributeError:\n",
    "        contents['isItFullTime'] = \"\"\n",
    "\n",
    "    # Description\n",
    "    try:\n",
    "        page_links = page_soup.find('div', {'id': 'Snippet'})\n",
    "        description = page_links.get_text().strip()\n",
    "        contents[\"description\"] = description\n",
    "    except AttributeError:\n",
    "        contents['description'] = \"\"\n",
    "\n",
    "    # Starting date\n",
    "    try:\n",
    "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__start-date py-2 pr-md-3 text-nowrap d-block d-md-inline-block'})\n",
    "        starting = page_links.get_text().strip()\n",
    "        contents[\"startDate\"] = starting\n",
    "    except AttributeError:\n",
    "        contents['startDate'] = \"\"\n",
    "\n",
    "    # Fees\n",
    "    try:\n",
    "        page_links = page_soup.find('div', {'class': 'course-sections course-sections__fees tight col-xs-24'})\n",
    "        fees = page_links.get_text().strip()\n",
    "        contents[\"fees\"] = fees\n",
    "    except AttributeError:\n",
    "        contents['fees'] = \"\"\n",
    "\n",
    "    # Modality\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'title': 'View all MSc courses'})\n",
    "        modality = page_links.get_text().strip()\n",
    "        contents[\"modality\"] = modality\n",
    "    except AttributeError:\n",
    "        contents['modality'] = \"\"\n",
    "\n",
    "    # Duration\n",
    "    try:\n",
    "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__duration py-2 pr-md-3 d-block d-md-inline-block'})\n",
    "        duration = page_links.get_text().strip()\n",
    "        contents[\"duration\"] = duration\n",
    "    except AttributeError:\n",
    "        contents['duration'] = \"\"\n",
    "\n",
    "    # City\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__city'})\n",
    "        city = page_links.get_text().strip()\n",
    "        contents[\"city\"] = city\n",
    "    except AttributeError:\n",
    "        contents['city'] = \"\"\n",
    "\n",
    "    # Country\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__country'})\n",
    "        country = page_links.get_text().strip()\n",
    "        contents[\"country\"] = country\n",
    "    except AttributeError:\n",
    "        contents['country'] = \"\"\n",
    "\n",
    "    # Administration\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__on-campus'})\n",
    "        administration = page_links.get_text().strip()\n",
    "        contents[\"administration\"] = administration\n",
    "    except AttributeError:\n",
    "        contents[\"administration\"] = \"\"\n",
    "\n",
    "    return contents\n",
    "\n",
    "\n",
    "#with open(\"html_pages/page_1/3d-design-for-virtual-environments-msc.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "#    html_content = file.read()\n",
    "\n",
    "#result = extract_msc_page_from_html(html_content)\n",
    "#print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'html_pages/'\n",
    "for n in range(1, 401):\n",
    "    file_name = f'page_{n}'\n",
    "    path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if the file exists before attempting to open it\n",
    "    if os.path.exists(path):\n",
    "        print(path)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "            extract_msc_page_from_html(html_content)\n",
    "    else:\n",
    "        print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'html_pages/'\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "# Walk through the directory and its subdirectories\n",
    "for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "    # Iterate through all the files in the current subdirectory\n",
    "    for filename in filenames:\n",
    "        # Construct the file path\n",
    "        path = os.path.join(foldername, filename)\n",
    "        \n",
    "        # Check if the file exists before attempting to open it\n",
    "        if os.path.exists(path):\n",
    "            # Print the file path\n",
    "            print(path)\n",
    "            \n",
    "            # Open the file in read mode with UTF-8 encoding\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                # Read the HTML content of the file\n",
    "                html_content = file.read()\n",
    "                \n",
    "                # Call the function to extract information from the HTML content\n",
    "                result_dict = extract_msc_page_from_html(html_content)\n",
    "                result_df = result_df.append(result_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Print a message if the file is not found\n",
    "            print(f\"File not found: {path}\")\n",
    "\n",
    "result_df.to_json('html_pages.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['fees'] = result_df['fees'].str.replace('Fees', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV files created successfully.\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"tsv_files/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate through each row and create a TSV file\n",
    "for index, row in result_df.iterrows():\n",
    "    file_name = f\"course_{index}.tsv\"\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    # Extract relevant columns and write to the TSV file\n",
    "    selected_columns = ['courseName', 'universityName', 'facultyName', 'isItFullTime', 'description', 'startDate',\n",
    "                        'fees','modality','duration','city','country','administration', 'url']\n",
    "    selected_data = row[selected_columns]\n",
    "    selected_data.to_csv(file_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "print(\"TSV files created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
