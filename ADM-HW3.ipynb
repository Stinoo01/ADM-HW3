{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bs4\n",
    "#pip install selenium\n",
    "#pip install forex-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required packages\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords # import stopwords module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False # run only once\n",
    "\n",
    "if flag:\n",
    "    # URL of the website\n",
    "    url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='  \n",
    "    prefix = '/masters-degrees/course/'\n",
    "    exclude = ['\\nMore details \\n', '\\nRead more \\n', '\\xa0Video(s)', '\\xa0Student Profile(s)'] \n",
    "\n",
    "    # Create a list to store the URLs of the masters\n",
    "    master_urls = []\n",
    "\n",
    "    # Loop through the first 400 pages\n",
    "    for page_number in range(1, 401):\n",
    "        print(url + str(page_number))\n",
    "        response = requests.get(url + str(page_number))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "            # Use BeautifulSoup to extract the URLs and append them to the master_urls list\n",
    "            for link in soup.find_all('a', {'class':'courseLink'}):\n",
    "                if link['href'][:len(prefix)] == prefix and not link.text in exclude:\n",
    "                    master_urls.append((link['href'], link.text))\n",
    "\n",
    "\n",
    "    # Save the collected URLs in a text file\n",
    "    with open(\"master_urls.txt\", \"a\") as file:\n",
    "        for url in master_urls:\n",
    "            file.write(url[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "\n",
    "if flag:\n",
    "    # Open the file and read its content\n",
    "    with open(\"master_urls.txt\", \"r\") as file:\n",
    "        master_urls = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    # Create a directory to store HTML pages\n",
    "    output_root_directory = \"html_pages\"\n",
    "    os.makedirs(output_root_directory, exist_ok=True)  # create the directory if it doen't exist\n",
    "\n",
    "    # Read 15 URLs at a time and create HTML pages\n",
    "    subset_size = 15\n",
    "    for i in range(0, len(master_urls), subset_size):\n",
    "        subset = master_urls[i:i + subset_size] # extract 15 more urls\n",
    "\n",
    "        # Create a subfolder for each page\n",
    "        output_directory = os.path.join(output_root_directory, f\"page_{i // subset_size + 1}\")\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Create an HTML page for each URL in the subset\n",
    "        for url in subset:\n",
    "            prefix = 'https://www.findamasters.com/'\n",
    "            response = requests.get(prefix + url)  # sends a GET request \n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                page_content = soup.prettify()  # extract the content of the page          \n",
    "                master_name = url.split(\"/\")[-2]  # extract the name of the master from the URL \n",
    "\n",
    "                # Check if the file already exists, and append a number if necessary\n",
    "                page_filename = f\"{output_directory}/{master_name}.html\"\n",
    "                counter = 1\n",
    "                while os.path.exists(page_filename):\n",
    "                    page_filename = f\"{output_directory}/{master_name}({counter}).html\"\n",
    "                    counter += 1\n",
    "\n",
    "                # Save the content in a HTML file\n",
    "                with open(page_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                    file.write(page_content)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        print(f\"page {i // subset_size + 1} completed\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MY IDEA --> I have to create a function that retrieve the required information from a html page. in order to do so I exploit the structure of the file. \n",
    "#I use \"try: except:\" to avoid errors in the case that some information is missing\n",
    "\n",
    "def extract_msc_page(msc_page_url):\n",
    "\n",
    "\n",
    "    contents ={}\n",
    "    # Parse HTML content\n",
    "    page_soup = BeautifulSoup(msc_page_url, 'html.parser')\n",
    "\n",
    "    #url\n",
    "    try:\n",
    "        canonical_link = page_soup.find('link', {'rel': 'canonical'})\n",
    "        contents['url'] = canonical_link.get('href')\n",
    "    except AttributeError:\n",
    "        contents['url'] = \"\"\n",
    "\n",
    "\n",
    "    # Course name\n",
    "    try:\n",
    "        page_links = page_soup.find('h1', {'class': 'text-white course-header__course-title'})\n",
    "        name = page_links.get_text()\n",
    "        contents[\"courseName\"] = name.strip()\n",
    "    except AttributeError:\n",
    "        contents['courseName'] = \"\"\n",
    "\n",
    "    # University name\n",
    "    try:\n",
    "        page_links = page_soup.find_all('a', {'class': 'course-header__institution'})\n",
    "        contents['universityName'] = page_links[0].contents[0].strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        contents['universityName'] = \"\"\n",
    "\n",
    "    # Faculty name\n",
    "    try:\n",
    "        page_links = page_soup.find_all('a', {'class': 'course-header__department'})\n",
    "        contents['facultyName'] = page_links[0].contents[0].strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        contents['facultyName'] = \"\"\n",
    "\n",
    "    # Full time\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'inheritFont concealLink text-decoration-none text-gray-600'})\n",
    "        time = page_links.get_text().strip()\n",
    "        contents['isItFullTime'] = time\n",
    "    except AttributeError:\n",
    "        contents['isItFullTime'] = \"\"\n",
    "\n",
    "    # Description\n",
    "    try:\n",
    "        page_links = page_soup.find('div', {'id': 'Snippet'})\n",
    "        description = page_links.get_text().strip()\n",
    "        contents[\"description\"] = description\n",
    "    except AttributeError:\n",
    "        contents['description'] = \"\"\n",
    "\n",
    "    # Starting date\n",
    "    try:\n",
    "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__start-date py-2 pr-md-3 text-nowrap d-block d-md-inline-block'})\n",
    "        starting = page_links.get_text().strip()\n",
    "        contents[\"startDate\"] = starting\n",
    "    except AttributeError:\n",
    "        contents['startDate'] = \"\"\n",
    "\n",
    "    # Fees\n",
    "    try:\n",
    "        page_links = page_soup.find('div', {'class': 'course-sections course-sections__fees tight col-xs-24'})\n",
    "        fees = page_links.get_text().strip()\n",
    "        contents[\"fees\"] = fees\n",
    "    except AttributeError:\n",
    "        contents['fees'] = \"\"\n",
    "\n",
    "    # Modality\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'title': 'View all MSc courses'})\n",
    "        modality = page_links.get_text().strip()\n",
    "        contents[\"modality\"] = modality\n",
    "    except AttributeError:\n",
    "        contents['modality'] = \"\"\n",
    "\n",
    "    # Duration\n",
    "    try:\n",
    "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__duration py-2 pr-md-3 d-block d-md-inline-block'})\n",
    "        duration = page_links.get_text().strip()\n",
    "        contents[\"duration\"] = duration\n",
    "    except AttributeError:\n",
    "        contents['duration'] = \"\"\n",
    "\n",
    "    # City\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__city'})\n",
    "        city = page_links.get_text().strip()\n",
    "        contents[\"city\"] = city\n",
    "    except AttributeError:\n",
    "        contents['city'] = \"\"\n",
    "\n",
    "    # Country\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__country'})\n",
    "        country = page_links.get_text().strip()\n",
    "        contents[\"country\"] = country\n",
    "    except AttributeError:\n",
    "        contents['country'] = \"\"\n",
    "\n",
    "    # Administration\n",
    "    try:\n",
    "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__on-campus'})\n",
    "        administration = page_links.get_text().strip()\n",
    "        contents[\"administration\"] = administration\n",
    "    except AttributeError:\n",
    "        contents[\"administration\"] = \"\"\n",
    "\n",
    "    \n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MY IDEA --> I apply the function above for all the files. I am applying it for all files in a folder for all folders contained into a folder\n",
    "\n",
    "flag = False\n",
    "\n",
    "if flag:\n",
    "    folder_path = 'html_pages/'\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "        # Iterate through all the files in the current subdirectory\n",
    "        for filename in filenames:\n",
    "            # Construct the file path\n",
    "            path = os.path.join(foldername, filename)\n",
    "        \n",
    "            # Check if the file exists before attempting to open it\n",
    "            if os.path.exists(path):\n",
    "                # Print the file path\n",
    "                print(path)\n",
    "            \n",
    "                # Open the file in read mode with UTF-8 encoding\n",
    "                with open(path, 'r', encoding='utf-8') as file:\n",
    "                    # Read the HTML content of the file\n",
    "                    html_content = file.read()\n",
    "                \n",
    "                    # Call the function to extract information from the HTML content\n",
    "                    result_dict = extract_msc_page(html_content)\n",
    "                    result_df = result_df.append(result_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Print a message if the file is not found\n",
    "                print(f\"File not found: {path}\")\n",
    "\n",
    "\n",
    "    #clean an imperfection in the fees section\n",
    "    result_df['fees'] = result_df['fees'].str.replace('Fees', '') \n",
    "\n",
    "    #save it into a file to store it. so that I do not have to run it again\n",
    "    result_df.to_json('html_pages.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tsv files \n",
    "\n",
    "flag = False\n",
    "\n",
    "if flag:\n",
    "    data = pd.read_json(\"html_pages.json\", lines=True)\n",
    "\n",
    "    output_directory = \"tsv_files/\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through each row and create a TSV file\n",
    "    for index, row in data.iterrows():\n",
    "        file_name = f\"course_{index}.tsv\"\n",
    "        file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        # Extract relevant columns and write to the TSV file\n",
    "        selected_columns = ['courseName', 'universityName', 'facultyName', 'isItFullTime', 'description', 'startDate',\n",
    "                        'fees','modality','duration','city','country','administration', 'url']\n",
    "        selected_data = row[selected_columns]\n",
    "        selected_data.to_csv(file_path, sep='\\t', index=False, header=False)\n",
    "\n",
    "    print(\"TSV files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.0.0) Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_json(\"html_pages.json\", lines=True)  # read the dataset from the created json file\n",
    "dataset = dataset[dataset.description != '']  # filter rows where the 'description' is empty\n",
    "\n",
    "# STEMMING\n",
    "stemmer = PorterStemmer()  # create an instance of Porter Stemmer\n",
    "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in row.split(' ')])  # reduce words of description column to their root form and create a new column to store the result\n",
    "\n",
    "# STOPWORDS\n",
    "nltk.download('stopwords') \n",
    "list_stopwords = stopwords.words('english')  # retrieves the English stopwords from the nltk stopwords dataset\n",
    "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in row.split(' ') if not word in list_stopwords])  # now 'descr_clean' column contains lists of cleaned and stemmed words \n",
    "\n",
    "# PUNCTUATION\n",
    "nltk.download('punkt')\n",
    "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in nltk.word_tokenize(row) if not word in list_stopwords and word.isalnum()]) # now 'descr_clean' column contains lists of cleaned and stemmed words without punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1) Preprocessing the fees column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "pattern1 = r'([$£¥€]|USD|GBP|EUR|euro|euros|JPY|HK$|HK|HKD$|HKD|ISK|SEK)\\s*([\\d,]+(?:\\.\\d+)?)'  # regex to obtain matches as \"symbol number\"\n",
    "pattern2 = r'([\\d,]+(?:\\.\\d+)?)\\s*([$£¥€]|USD|GBP|EUR|euro|euros|JPY|HK$|HK|HKD$|HKD|ISK|SEK)'  # regex to obtain matches as \"number symbol\"\n",
    "\n",
    "# Create a CurrencyRates instance\n",
    "currency_rates = CurrencyRates()\n",
    "\n",
    "def extract_and_convert(row, common_currency=\"USD\"):\n",
    "\n",
    "    matches1 = re.finditer(pattern1, row)\n",
    "    matches2 = re.finditer(pattern2, row)\n",
    "\n",
    "    max_value = 0\n",
    "    max_currency = None\n",
    "    max_match = None\n",
    "\n",
    "    \n",
    "    for match in matches1:\n",
    "        currency_symbol = match.group(1)\n",
    "        numeric_part = match.group(2)\n",
    "        \n",
    "        \n",
    "        # Try to convert numeric part to float\n",
    "        try:\n",
    "            numeric_value = float(numeric_part.replace(',', '').replace('.', ''))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        if numeric_value > max_value:  # update max value if necessary\n",
    "            max_value = numeric_value\n",
    "            max_currency = currency_symbol\n",
    "            max_match = match\n",
    "    \n",
    "    for match in matches2:\n",
    "        currency_symbol = match.group(2)\n",
    "        numeric_part = match.group(1)\n",
    "        \n",
    "        \n",
    "        # Try to convert numeric part to float\n",
    "        try:\n",
    "            numeric_value = float(numeric_part.replace(',', '').replace('.', '')) # remove all the number separators to do the cast\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        if numeric_value > max_value: # update max value if necessary\n",
    "            max_value = numeric_value\n",
    "            max_currency = currency_symbol\n",
    "            max_match = match\n",
    "    \n",
    "    if max_match:\n",
    "        # Convert the price to the common currency\n",
    "        converted_price = convert_currency(max_currency, max_value, common_currency)\n",
    "        return converted_price\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define a function to convert currency\n",
    "def convert_currency(currency_symbol, amount, target_currency):\n",
    "    try:\n",
    "        # Map currency symbols to ISO codes\n",
    "        currency_code = symbol_to_code(currency_symbol)\n",
    "        \n",
    "        if currency_code is not None:\n",
    "            # Use the forex-python library to get the exchange rate and perform the conversion\n",
    "            exchange_rate = currency_rates.get_rate(currency_code, target_currency)\n",
    "            converted_amount = round(amount * exchange_rate, 2)\n",
    "            print(f\"Previous price: {amount}{currency_symbol}\")\n",
    "            print(f\"Converted Price: {converted_amount:.2f} USD\")\n",
    "            return converted_amount\n",
    "        else:\n",
    "            print(f\"Unsupported currency symbol: {currency_symbol}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting currency: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define a function to map currency symbols to ISO codes\n",
    "def symbol_to_code(symbol):\n",
    "    symbol_map = {\n",
    "        '$': 'USD',\n",
    "        '£': 'GBP',\n",
    "        '¥': 'JPY',\n",
    "        '€': 'EUR',\n",
    "        'USD': 'USD',\n",
    "        'GBP': 'GBP',\n",
    "        'EUR': 'EUR',\n",
    "        'euro': 'EUR',\n",
    "        'euros': 'EUR',\n",
    "        'JPY': 'JPY',\n",
    "        'HKD$': 'HKD',\n",
    "        'HKD': 'HKD',\n",
    "        'HK$': 'HKD',\n",
    "        'HK': 'HKD',\n",
    "        'ISK': 'ISK',\n",
    "        'SEK': 'SEK'\n",
    "    }\n",
    "    return symbol_map.get(symbol, None)\n",
    "\n",
    "\n",
    "# Apply the function to the 'fees' column and create a new 'fees(USD)' column\n",
    "dataset['fees(USD)'] = dataset['fees'].apply(lambda x: extract_and_convert(x, common_currency='USD'))\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1:\n",
    "#Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "\n",
    "vocabulary = set()\n",
    "dataset.preprocessed_description.apply(lambda row: [vocabulary.add(word) for word in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_alt = Counter(reduce(lambda x, y: x + y, dataset.preprocessed_description.values)).keys()\n",
    "\n",
    "index = {}\n",
    "unique_id = 1\n",
    "for word in list(vocabulary_alt):\n",
    "  index[unique_id] = word\n",
    "  unique_id += 1\n",
    "\n",
    "# Create a DataFrame from the vocabulary\n",
    "vocabulary_df_alt = pd.DataFrame(list(index.items()), columns=['term_id', 'word'])\n",
    "\n",
    "# Save the vocabulary DataFrame to a CSV file\n",
    "vocabulary_df_alt.to_csv('vocabulary_alt.csv', index=False)\n",
    "\n",
    "\n",
    "terms = pd.DataFrame(data=list(vocabulary_alt), columns=['term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalized_query = str(input())\n",
    "\n",
    "#personalized_query = \"3d group\"\n",
    "\n",
    "# Split the personalized query into individual words\n",
    "query_words = personalized_query.split()\n",
    "\n",
    "# Your modified code to get the indices of documents containing all words in the personalized query\n",
    "selected_indices = list(dataset.loc[dataset.preprocessed_description.apply(lambda row: all(word in row for word in query_words))].index)\n",
    "\n",
    "# Extract information for each selected document\n",
    "selected_documents = dataset.loc[selected_indices, ['courseName', 'universityName', 'description', 'url']]\n",
    "\n",
    "# Print the information for each selected document\n",
    "print(selected_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(input = \"content\", lowercase= False, tokenizer = lambda text: text, max_df=0.5, min_df = 2)\n",
    "\n",
    "results = tfidf.fit_transform(dataset.preprocessed_description)\n",
    "result_dense = results.todense()\n",
    "pd.DataFrame(result_dense.tolist(), index=dataset.index, columns=tfidf.get_feature_names_out())\n",
    "\n",
    "\n",
    "tfidf_data = pd.DataFrame(result_dense.tolist(), index=dataset.index, columns=tfidf.get_feature_names_out())\n",
    "cosine_sim = cosine_similarity(tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(query, k=5):\n",
    "    # Tokenize and preprocess the query\n",
    "    query_tfidf = tfidf.transform([query])\n",
    "\n",
    "    # Compute cosine similarity between the query and all documents\n",
    "    query_similarity = cosine_similarity(query_tfidf, tfidf_data)\n",
    "\n",
    "    # Create a heap to maintain the top-k documents\n",
    "    heap = []\n",
    "\n",
    "    # Iterate through the documents and update the heap\n",
    "    for i, sim_score in enumerate(query_similarity[0]):\n",
    "        heapq.heappush(heap, (sim_score, i))\n",
    "\n",
    "    # Retrieve the top-k documents from the heap\n",
    "    top_k_documents = []\n",
    "    while heap and len(top_k_documents) < k:\n",
    "        sim_score, index = heapq.heappop(heap)\n",
    "        if sim_score > 0:\n",
    "            document_info = {\n",
    "                'courseName': dataset.loc[index, 'courseName'],\n",
    "                'universityName': dataset.loc[index, 'universityName'],\n",
    "                'description': dataset.loc[index, 'description'],\n",
    "                'url': dataset.loc[index, 'url'],\n",
    "                'Similarity Score': sim_score\n",
    "            }\n",
    "            top_k_documents.append(document_info)\n",
    "\n",
    "    result_df = pd.DataFrame(top_k_documents)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING IT BY CHANGING THE CODE\n",
    "query = \"advanced knowledge\"\n",
    "top_k_results = search_engine(query, k=5)\n",
    "\n",
    "\n",
    "#USING IT BY DOING AN INPUT.\n",
    "personalized_query = str(input())\n",
    "top_k_results = search_engine(personalized_query, k=5)\n",
    "\n",
    "print(top_k_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
