{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJp8oLahEwyE"
      },
      "source": [
        "### 1. DATA COLLECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWvDHliwEwyJ"
      },
      "outputs": [],
      "source": [
        "#pip install bs4\n",
        "#pip install selenium\n",
        "#pip install forex-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "71NgqMEPEwyL"
      },
      "outputs": [],
      "source": [
        "#import the required packages\n",
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords # import stopwords module\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFuPcQYREwyN"
      },
      "source": [
        "#### 1.1. Get the list of master's degree courses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA9N0hEQEwyP"
      },
      "outputs": [],
      "source": [
        "flag = False # run only once\n",
        "\n",
        "if flag:\n",
        "    # URL of the website\n",
        "    url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='\n",
        "    prefix = '/masters-degrees/course/'\n",
        "    exclude = ['\\nMore details \\n', '\\nRead more \\n', '\\xa0Video(s)', '\\xa0Student Profile(s)']\n",
        "\n",
        "    # Create a list to store the URLs of the masters\n",
        "    master_urls = []\n",
        "\n",
        "    # Loop through the first 400 pages\n",
        "    for page_number in range(1, 401):\n",
        "        print(url + str(page_number))\n",
        "        response = requests.get(url + str(page_number))\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Use BeautifulSoup to extract the URLs and append them to the master_urls list\n",
        "            for link in soup.find_all('a', {'class':'courseLink'}):\n",
        "                if link['href'][:len(prefix)] == prefix and not link.text in exclude:\n",
        "                    master_urls.append((link['href'], link.text))\n",
        "\n",
        "\n",
        "    # Save the collected URLs in a text file\n",
        "    with open(\"master_urls.txt\", \"a\") as file:\n",
        "        for url in master_urls:\n",
        "            file.write(url[0] + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gny_6ebiEwyR"
      },
      "source": [
        "#### 1.2. Crawl master's degree pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOQMrMEGEwyS"
      },
      "outputs": [],
      "source": [
        "flag = False\n",
        "\n",
        "if flag:\n",
        "    # Open the file and read its content\n",
        "    with open(\"master_urls.txt\", \"r\") as file:\n",
        "        master_urls = [line.strip() for line in file.readlines()]\n",
        "\n",
        "    # Create a directory to store HTML pages\n",
        "    output_root_directory = \"html_pages\"\n",
        "    os.makedirs(output_root_directory, exist_ok=True)  # create the directory if it doen't exist\n",
        "\n",
        "    # Read 15 URLs at a time and create HTML pages\n",
        "    subset_size = 15\n",
        "    for i in range(0, len(master_urls), subset_size):\n",
        "        subset = master_urls[i:i + subset_size] # extract 15 more urls\n",
        "\n",
        "        # Create a subfolder for each page\n",
        "        output_directory = os.path.join(output_root_directory, f\"page_{i // subset_size + 1}\")\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "        # Create an HTML page for each URL in the subset\n",
        "        for url in subset:\n",
        "            prefix = 'https://www.findamasters.com/'\n",
        "            response = requests.get(prefix + url)  # sends a GET request\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "                page_content = soup.prettify()  # extract the content of the page\n",
        "                master_name = url.split(\"/\")[-2]  # extract the name of the master from the URL\n",
        "\n",
        "                # Check if the file already exists, and append a number if necessary\n",
        "                page_filename = f\"{output_directory}/{master_name}.html\"\n",
        "                counter = 1\n",
        "                while os.path.exists(page_filename):\n",
        "                    page_filename = f\"{output_directory}/{master_name}({counter}).html\"\n",
        "                    counter += 1\n",
        "\n",
        "                # Save the content in a HTML file\n",
        "                with open(page_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "                    file.write(page_content)\n",
        "            time.sleep(1.5)\n",
        "\n",
        "        print(f\"page {i // subset_size + 1} completed\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnK6X0fuEwyT"
      },
      "source": [
        "#### 1.3 Parse downloaded pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZQYRqnlEwyU"
      },
      "outputs": [],
      "source": [
        "##MY IDEA --> I have to create a function that retrieve the required information from a html page. in order to do so I exploit the structure of the file.\n",
        "#I use \"try: except:\" to avoid errors in the case that some information is missing\n",
        "\n",
        "def extract_msc_page(msc_page_url):\n",
        "\n",
        "\n",
        "    contents ={}\n",
        "    # Parse HTML content\n",
        "    page_soup = BeautifulSoup(msc_page_url, 'html.parser')\n",
        "\n",
        "    #url\n",
        "    try:\n",
        "        canonical_link = page_soup.find('link', {'rel': 'canonical'})\n",
        "        contents['url'] = canonical_link.get('href')\n",
        "    except AttributeError:\n",
        "        contents['url'] = \"\"\n",
        "\n",
        "\n",
        "    # Course name\n",
        "    try:\n",
        "        page_links = page_soup.find('h1', {'class': 'text-white course-header__course-title'})\n",
        "        name = page_links.get_text()\n",
        "        contents[\"courseName\"] = name.strip()\n",
        "    except AttributeError:\n",
        "        contents['courseName'] = \"\"\n",
        "\n",
        "    # University name\n",
        "    try:\n",
        "        page_links = page_soup.find_all('a', {'class': 'course-header__institution'})\n",
        "        contents['universityName'] = page_links[0].contents[0].strip()\n",
        "    except (AttributeError, IndexError):\n",
        "        contents['universityName'] = \"\"\n",
        "\n",
        "    # Faculty name\n",
        "    try:\n",
        "        page_links = page_soup.find_all('a', {'class': 'course-header__department'})\n",
        "        contents['facultyName'] = page_links[0].contents[0].strip()\n",
        "    except (AttributeError, IndexError):\n",
        "        contents['facultyName'] = \"\"\n",
        "\n",
        "    # Full time\n",
        "    try:\n",
        "        page_links = page_soup.find('a', {'class': 'inheritFont concealLink text-decoration-none text-gray-600'})\n",
        "        time = page_links.get_text().strip()\n",
        "        contents['isItFullTime'] = time\n",
        "    except AttributeError:\n",
        "        contents['isItFullTime'] = \"\"\n",
        "\n",
        "    # Description\n",
        "    try:\n",
        "        page_links = page_soup.find('div', {'id': 'Snippet'})\n",
        "        description = page_links.get_text().strip()\n",
        "        contents[\"description\"] = description\n",
        "    except AttributeError:\n",
        "        contents['description'] = \"\"\n",
        "\n",
        "    # Starting date\n",
        "    try:\n",
        "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__start-date py-2 pr-md-3 text-nowrap d-block d-md-inline-block'})\n",
        "        starting = page_links.get_text().strip()\n",
        "        contents[\"startDate\"] = starting\n",
        "    except AttributeError:\n",
        "        contents['startDate'] = \"\"\n",
        "\n",
        "    # Fees\n",
        "    try:\n",
        "        page_links = page_soup.find('div', {'class': 'course-sections course-sections__fees tight col-xs-24'})\n",
        "        fees = page_links.get_text().strip()\n",
        "        contents[\"fees\"] = fees\n",
        "    except AttributeError:\n",
        "        contents['fees'] = \"\"\n",
        "\n",
        "    # Modality\n",
        "    try:\n",
        "        page_links = page_soup.find('a', {'title': 'View all MSc courses'})\n",
        "        modality = page_links.get_text().strip()\n",
        "        contents[\"modality\"] = modality\n",
        "    except AttributeError:\n",
        "        contents['modality'] = \"\"\n",
        "\n",
        "    # Duration\n",
        "    try:\n",
        "        page_links = page_soup.find('span', {'class': 'key-info__content key-info__duration py-2 pr-md-3 d-block d-md-inline-block'})\n",
        "        duration = page_links.get_text().strip()\n",
        "        contents[\"duration\"] = duration\n",
        "    except AttributeError:\n",
        "        contents['duration'] = \"\"\n",
        "\n",
        "    # City\n",
        "    try:\n",
        "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__city'})\n",
        "        city = page_links.get_text().strip()\n",
        "        contents[\"city\"] = city\n",
        "    except AttributeError:\n",
        "        contents['city'] = \"\"\n",
        "\n",
        "    # Country\n",
        "    try:\n",
        "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__country'})\n",
        "        country = page_links.get_text().strip()\n",
        "        contents[\"country\"] = country\n",
        "    except AttributeError:\n",
        "        contents['country'] = \"\"\n",
        "\n",
        "    # Administration\n",
        "    try:\n",
        "        page_links = page_soup.find('a', {'class': 'card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__on-campus'})\n",
        "        administration = page_links.get_text().strip()\n",
        "        contents[\"administration\"] = administration\n",
        "    except AttributeError:\n",
        "        contents[\"administration\"] = \"\"\n",
        "\n",
        "\n",
        "    return contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-AOoS7UEwyW"
      },
      "outputs": [],
      "source": [
        "## MY IDEA --> I apply the function above for all the files. I am applying it for all files in a folder for all folders contained into a folder\n",
        "\n",
        "flag = False\n",
        "\n",
        "if flag:\n",
        "    folder_path = 'html_pages/'\n",
        "\n",
        "    result_df = pd.DataFrame()\n",
        "    # Walk through the directory and its subdirectories\n",
        "    for foldername, subfolders, filenames in os.walk(folder_path):\n",
        "        # Iterate through all the files in the current subdirectory\n",
        "        for filename in filenames:\n",
        "            # Construct the file path\n",
        "            path = os.path.join(foldername, filename)\n",
        "\n",
        "            # Check if the file exists before attempting to open it\n",
        "            if os.path.exists(path):\n",
        "                # Print the file path\n",
        "                print(path)\n",
        "\n",
        "                # Open the file in read mode with UTF-8 encoding\n",
        "                with open(path, 'r', encoding='utf-8') as file:\n",
        "                    # Read the HTML content of the file\n",
        "                    html_content = file.read()\n",
        "\n",
        "                    # Call the function to extract information from the HTML content\n",
        "                    result_dict = extract_msc_page(html_content)\n",
        "                    result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "\n",
        "            else:\n",
        "                # Print a message if the file is not found\n",
        "                print(f\"File not found: {path}\")\n",
        "\n",
        "\n",
        "    #clean an imperfection in the fees section\n",
        "    result_df['fees'] = result_df['fees'].str.replace('Fees', '')\n",
        "\n",
        "    #save it into a file to store it. so that I do not have to run it again\n",
        "    result_df.to_json('html_pages.json', orient='records', lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzwwkIwvEwyY"
      },
      "outputs": [],
      "source": [
        "#create the tsv files\n",
        "\n",
        "flag = False\n",
        "\n",
        "if flag:\n",
        "    data = pd.read_json(\"html_pages.json\", lines=True)\n",
        "\n",
        "    output_directory = \"tsv_files/\"\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Iterate through each row and create a TSV file\n",
        "    for index, row in data.iterrows():\n",
        "        file_name = f\"course_{index}.tsv\"\n",
        "        file_path = os.path.join(output_directory, file_name)\n",
        "\n",
        "        # Extract relevant columns and write to the TSV file\n",
        "        selected_columns = ['courseName', 'universityName', 'facultyName', 'isItFullTime', 'description', 'startDate',\n",
        "                        'fees','modality','duration','city','country','administration', 'url']\n",
        "        selected_data = row[selected_columns]\n",
        "        selected_data.to_csv(file_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "    print(\"TSV files created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUgsbKRVEwya"
      },
      "source": [
        "### 2. Search Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3zQucRwEwya"
      },
      "source": [
        "##### 2.0.0) Preprocessing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyvNFXyMEwyb",
        "outputId": "2291b751-1768-4f3d-b815-7c0b4aa2070b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Mario\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_json(\"html_pages.json\", lines=True)  # read the dataset from the created json file\n",
        "dataset = dataset[dataset.description != '']  # filter rows where the 'description' is empty\n",
        "\n",
        "# STEMMING\n",
        "stemmer = PorterStemmer()  # create an instance of Porter Stemmer\n",
        "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in row.split(' ')])  # reduce words of description column to their root form and create a new column to store the result\n",
        "\n",
        "# STOPWORDS\n",
        "nltk.download('stopwords')\n",
        "list_stopwords = stopwords.words('english')  # retrieves the English stopwords from the nltk stopwords dataset\n",
        "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in row.split(' ') if not word in list_stopwords])  # now 'descr_clean' column contains lists of cleaned and stemmed words\n",
        "\n",
        "# PUNCTUATION\n",
        "nltk.download('punkt')\n",
        "dataset['preprocessed_description'] = dataset.description.apply(lambda row: [stemmer.stem(word) for word in nltk.word_tokenize(row) if not word in list_stopwords and word.isalnum()]) # now 'descr_clean' column contains lists of cleaned and stemmed words without punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjyo5dAXEwye"
      },
      "source": [
        "#### 2.0.1) Preprocessing the fees column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRI0-kdiEwyf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from forex_python.converter import CurrencyRates\n",
        "\n",
        "pattern1 = r'([$£¥€]|USD|GBP|EUR|euro|euros|JPY|HK$|HK|HKD$|HKD|ISK|SEK)\\s*([\\d,]+(?:\\.\\d+)?)'  # regex to obtain matches as \"symbol number\"\n",
        "pattern2 = r'([\\d,]+(?:\\.\\d+)?)\\s*([$£¥€]|USD|GBP|EUR|euro|euros|JPY|HK$|HK|HKD$|HKD|ISK|SEK)'  # regex to obtain matches as \"number symbol\"\n",
        "\n",
        "# Create a CurrencyRates instance\n",
        "currency_rates = CurrencyRates()\n",
        "\n",
        "def extract_and_convert(row, common_currency=\"USD\"):\n",
        "\n",
        "    matches1 = re.finditer(pattern1, row)\n",
        "    matches2 = re.finditer(pattern2, row)\n",
        "\n",
        "    max_value = 0\n",
        "    max_currency = None\n",
        "    max_match = None\n",
        "\n",
        "\n",
        "    for match in matches1:\n",
        "        currency_symbol = match.group(1)\n",
        "        numeric_part = match.group(2)\n",
        "\n",
        "\n",
        "        # Try to convert numeric part to float\n",
        "        try:\n",
        "            numeric_value = float(numeric_part.replace(',', '').replace('.', ''))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "        if numeric_value > max_value:  # update max value if necessary\n",
        "            max_value = numeric_value\n",
        "            max_currency = currency_symbol\n",
        "            max_match = match\n",
        "\n",
        "    for match in matches2:\n",
        "        currency_symbol = match.group(2)\n",
        "        numeric_part = match.group(1)\n",
        "\n",
        "\n",
        "        # Try to convert numeric part to float\n",
        "        try:\n",
        "            numeric_value = float(numeric_part.replace(',', '').replace('.', '')) # remove all the number separators to do the cast\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "        if numeric_value > max_value: # update max value if necessary\n",
        "            max_value = numeric_value\n",
        "            max_currency = currency_symbol\n",
        "            max_match = match\n",
        "\n",
        "    if max_match:\n",
        "        # Convert the price to the common currency\n",
        "        converted_price = convert_currency(max_currency, max_value, common_currency)\n",
        "        return converted_price\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Define a function to convert currency\n",
        "def convert_currency(currency_symbol, amount, target_currency):\n",
        "    try:\n",
        "        # Map currency symbols to ISO codes\n",
        "        currency_code = symbol_to_code(currency_symbol)\n",
        "\n",
        "        if currency_code is not None:\n",
        "            # Use the forex-python library to get the exchange rate and perform the conversion\n",
        "            exchange_rate = currency_rates.get_rate(currency_code, target_currency)\n",
        "            converted_amount = round(amount * exchange_rate, 2)\n",
        "            print(f\"Previous price: {amount}{currency_symbol}\")\n",
        "            print(f\"Converted Price: {converted_amount:.2f} USD\")\n",
        "            return converted_amount\n",
        "        else:\n",
        "            print(f\"Unsupported currency symbol: {currency_symbol}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting currency: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define a function to map currency symbols to ISO codes\n",
        "def symbol_to_code(symbol):\n",
        "    symbol_map = {\n",
        "        '$': 'USD',\n",
        "        '£': 'GBP',\n",
        "        '¥': 'JPY',\n",
        "        '€': 'EUR',\n",
        "        'USD': 'USD',\n",
        "        'GBP': 'GBP',\n",
        "        'EUR': 'EUR',\n",
        "        'euro': 'EUR',\n",
        "        'euros': 'EUR',\n",
        "        'JPY': 'JPY',\n",
        "        'HKD$': 'HKD',\n",
        "        'HKD': 'HKD',\n",
        "        'HK$': 'HKD',\n",
        "        'HK': 'HKD',\n",
        "        'ISK': 'ISK',\n",
        "        'SEK': 'SEK'\n",
        "    }\n",
        "    return symbol_map.get(symbol, None)\n",
        "\n",
        "\n",
        "# Apply the function to the 'fees' column and create a new 'fees(USD)' column\n",
        "dataset['fees(USD)'] = dataset['fees'].apply(lambda x: extract_and_convert(x, common_currency='USD'))\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uez4NuDlEwyh"
      },
      "source": [
        "### 2.1. Conjunctive query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snW2Hs1mEwyh"
      },
      "source": [
        "#### 2.1.1) Create your index!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgogtannEwyi"
      },
      "outputs": [],
      "source": [
        "#step 1:\n",
        "#Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
        "\n",
        "vocabulary = set()\n",
        "dataset.preprocessed_description.apply(lambda row: [vocabulary.add(word) for word in row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3BJn8TcEwyj"
      },
      "outputs": [],
      "source": [
        "vocabulary_alt = Counter(reduce(lambda x, y: x + y, dataset.preprocessed_description.values)).keys()\n",
        "\n",
        "index = {}\n",
        "unique_id = 1\n",
        "for word in list(vocabulary_alt):\n",
        "  index[unique_id] = word\n",
        "  unique_id += 1\n",
        "\n",
        "# Create a DataFrame from the vocabulary\n",
        "vocabulary_df_alt = pd.DataFrame(list(index.items()), columns=['term_id', 'word'])\n",
        "\n",
        "# Save the vocabulary DataFrame to a CSV file\n",
        "vocabulary_df_alt.to_csv('vocabulary_alt.csv', index=False)\n",
        "\n",
        "\n",
        "terms = pd.DataFrame(data=list(vocabulary_alt), columns=['term'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPIHZoChEwyj"
      },
      "source": [
        "#### 2.1.2) Execute the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bgmWp2QEwyj"
      },
      "outputs": [],
      "source": [
        "personalized_query = str(input())\n",
        "\n",
        "#personalized_query = \"3d group\"\n",
        "\n",
        "# Split the personalized query into individual words\n",
        "query_words = personalized_query.split()\n",
        "\n",
        "# Your modified code to get the indices of documents containing all words in the personalized query\n",
        "selected_indices = list(dataset.loc[dataset.preprocessed_description.apply(lambda row: all(word in row for word in query_words))].index)\n",
        "\n",
        "# Extract information for each selected document\n",
        "selected_documents = dataset.loc[selected_indices, ['courseName', 'universityName', 'description', 'url']]\n",
        "\n",
        "# Print the information for each selected document\n",
        "print(selected_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKSBnEzeEwyk"
      },
      "source": [
        "### 2.2) Conjunctive query & Ranking score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek9VbglREwyk"
      },
      "source": [
        "#### 2.2.1) Inverted index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uopJlYb5Ewyl"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(input = \"content\", lowercase= False, tokenizer = lambda text: text, max_df=0.5, min_df = 2)\n",
        "\n",
        "results = tfidf.fit_transform(dataset.preprocessed_description)\n",
        "result_dense = results.todense()\n",
        "pd.DataFrame(result_dense.tolist(), index=dataset.index, columns=tfidf.get_feature_names_out())\n",
        "\n",
        "\n",
        "tfidf_data = pd.DataFrame(result_dense.tolist(), index=dataset.index, columns=tfidf.get_feature_names_out())\n",
        "cosine_sim = cosine_similarity(tfidf_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfxuH0_lEwyl"
      },
      "outputs": [],
      "source": [
        "def search_engine(query, k=5):\n",
        "    # Tokenize and preprocess the query\n",
        "    query_tfidf = tfidf.transform([query])\n",
        "\n",
        "    # Compute cosine similarity between the query and all documents\n",
        "    query_similarity = cosine_similarity(query_tfidf, tfidf_data)\n",
        "\n",
        "    # Create a heap to maintain the top-k documents\n",
        "    heap = []\n",
        "\n",
        "    # Iterate through the documents and update the heap\n",
        "    for i, sim_score in enumerate(query_similarity[0]):\n",
        "        heapq.heappush(heap, (sim_score, i))\n",
        "\n",
        "    # Retrieve the top-k documents from the heap\n",
        "    top_k_documents = []\n",
        "    while heap and len(top_k_documents) < k:\n",
        "        sim_score, index = heapq.heappop(heap)\n",
        "        if sim_score > 0:\n",
        "            document_info = {\n",
        "                'courseName': dataset.loc[index, 'courseName'],\n",
        "                'universityName': dataset.loc[index, 'universityName'],\n",
        "                'description': dataset.loc[index, 'description'],\n",
        "                'url': dataset.loc[index, 'url'],\n",
        "                'Similarity Score': sim_score\n",
        "            }\n",
        "            top_k_documents.append(document_info)\n",
        "\n",
        "    result_df = pd.DataFrame(top_k_documents)\n",
        "\n",
        "    return result_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGYkQzagEwym"
      },
      "source": [
        "##### 2.2.2) Execute the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKq9CsW5Ewym"
      },
      "outputs": [],
      "source": [
        "#USING IT BY CHANGING THE CODE\n",
        "query = \"advanced knowledge\"\n",
        "top_k_results = search_engine(query, k=5)\n",
        "\n",
        "\n",
        "#USING IT BY DOING AN INPUT.\n",
        "personalized_query = str(input())\n",
        "top_k_results = search_engine(personalized_query, k=5)\n",
        "\n",
        "print(top_k_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVzSszJXEwyn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALGORITMIC QUESTION**"
      ],
      "metadata": {
        "id": "WAO-auEfFA3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Implement a code to solve the above mentioned problem.**\n",
        "\n"
      ],
      "metadata": {
        "id": "lvztsXIZE596"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_line = list(map(int, input().split()))\n",
        "d = first_line[0]\n",
        "sumHours = first_line[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKoME1aUE_Ih",
        "outputId": "5409958c-d5c7-4856-a185-d8211f86f5e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_times_list = []\n",
        "max_times_list = []\n",
        "\n",
        "error_flag = False #to ensure the input is in a logically correct format\n",
        "for i in range(d):\n",
        "  input_lines = list(map(int, input().split()))\n",
        "  if input_lines[0] > input_lines[1]:\n",
        "    print (\"Minimum Time Cannot Exceed Maximum Time\")\n",
        "    error_flag = True\n",
        "    break\n",
        "  else:\n",
        "    min_times_list.append(input_lines[0])\n",
        "    max_times_list.append(input_lines[1])\n",
        "\n",
        "if not error_flag: #if the input is in a logically correct format, we save the values to our lists.\n",
        "  min_times_list = min_times_list\n",
        "  max_times_list = max_times_list\n",
        "else: #if the input is not in a logically correct format, we do not save the input into our lists.\n",
        "  min_times_list = []\n",
        "  max_times_list = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIDaIjNtFMsz",
        "outputId": "e97b01e1-6f4a-4338-e9a1-0e764f69a053"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 1\n",
            "3 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worked_hours_list = []\n",
        "if (sum(min_times_list)>sumHours) or (len(min_times_list)==0): # the case which leads to an infeasable solution.\n",
        "  print (\"NO\")\n",
        "elif sum(min_times_list)==sumHours: # this part is only to save time if the total hour can only be achieved by using the minimum hours.\n",
        "  print (\"YES\")\n",
        "  print (min_times_list)\n",
        "else:\n",
        "  print (\"YES\")\n",
        "  while True: #to be able to loop until the following if condition is satisfied.\n",
        "    for i in range (d-1): #generate the hours for d-1 days randomly.\n",
        "      worked_hours_list.append(random.randint(min_times_list[i], max_times_list[i]))\n",
        "    current_sum = sum(worked_hours_list)\n",
        "    if ((sumHours - current_sum) >= min_times_list[d-1]) and ((sumHours - current_sum) <= max_times_list[d-1]): # logical condition to check if there is an answer with the random hours created so far.\n",
        "      worked_hours_list.append(sumHours - current_sum) #the last day's hour is equal to the difference between the total hour and the sum of the other days.\n",
        "      print (worked_hours_list)\n",
        "      break\n",
        "    else:\n",
        "      worked_hours_list = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4ugn28FTIN",
        "outputId": "cceb58b8-4cd3-4767-e190-0c4bdd8d99c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YES\n",
            "[1, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is the time complexity (the Big O notation) of your solution? Please provide a detailed explanation of how you calculated the time complexity.**"
      ],
      "metadata": {
        "id": "AtFnnQKPFZEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to calculate the time complexity, I mainly focused on the loops. We have the first \"for\" loop (for i in range(d)) which has the time complexity of O(n). The statements inside this \"for\" loop is repetated d times. Time complexity of all our \"if\" statements are equal to O(1), since they will be executed once. Then, the most important part, we have a \"for\" loop inside a \"while\" loop. The \"while\" loop will be executed until an \"if\" statement is true. In the best case, it will be executed only once. But in the worst case, the inside of the \"while\" loop will be executed indefinite amount. This is because there is the randomness element. Considering these, the time complexity cannot be calculated.\n"
      ],
      "metadata": {
        "id": "mkLSh62_Fa8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Ask ChatGPT or any other LLM chatbot tool to check your code's time complexity (the Big O notation). Compare your answer to theirs. Do you believe this is correct? If the two differ, which one is right? (why?)**\n"
      ],
      "metadata": {
        "id": "cR4shPlIFe8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer of ChatGPT:\n",
        "\n",
        "> \"Input Reading and Validation: The code reads input values, including the number of days (d), total hours (sumHours), and minimum and maximum hours for each day. The loop from for i in range(d) takes O(d) time, as it iterates over the number of days. Within this loop, it checks the validity of input and populates min_times_list and max_times_list.\n",
        "\n",
        "> Validation and Early Exit: There's a check (if not error_flag) to determine whether the input is valid. If it is, the code proceeds to the next steps. Otherwise, it exits early based on the input validation, which takes O(1) time.\n",
        "\n",
        "> Handling Infeasible Solutions: There are checks to handle cases where the total minimum hours are greater than the provided total hours or if the minimum times list is empty. These checks are done in O(d) time, as they involve checking the entire list.\n",
        "\n",
        "> Main Logic (While Loop): There's a while True loop, indicating an indefinite loop until a certain condition is met. This can potentially run multiple times, but the number of iterations is not fixed or determined solely by the input size. Inside this loop, there's a for loop iterating d - 1 times, generating random hours within certain ranges. This loop's time complexity is O(d) as it iterates over the number of days.\n",
        "\n",
        "> Condition Check and Final Output: Within the while loop, there's a check (if ...) involving arithmetic operations and comparisons. This condition is checked repeatedly until it satisfies certain criteria. The exact number of iterations of this check isn't straightforward to determine without knowing the randomness of the generated values.\n",
        "\n",
        "> Overall Time Complexity Analysis: The time complexity is mostly influenced by the loops that iterate based on the number of days (d). Input validation and handling infeasible solutions also contribute to the time complexity but in a linear manner. The while loop's time complexity, driven by randomness and the condition inside it, can vary significantly depending on the generated values, making it challenging to precisely determine the overall time complexity in terms of 'n' or 'd'. In conclusion, the time complexity of this code snippet is predominantly influenced by the iterations based on the number of days (d), but the overall complexity is challenging to express definitively due to the unpredictable nature of the loop's termination condition and the randomness involved in the generation of values.\"\n",
        "\n",
        "My answer and this answer are almost the same. ChatGPT's answer has more explanations but we both agree the overall time complexity cannot be calculated because of the randomness."
      ],
      "metadata": {
        "id": "uNF3aqjPFfO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What do you think of the optimality of your code? Do you believe it is optimal? Can you improve? Please elaborate on your response.**"
      ],
      "metadata": {
        "id": "UURO0UIaFfep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This depends on what Leonardo wants exactly. In our code, the main issue when it comes to optimality and time is about the randomness factor. But, it also makes the results more believable considering the hours will be examined by the HR. Other than this, we tried our code with various different inputs in order to check if it works correctly and concluded that it does. It is also easily maintable since the parameters are given by the user, so it is easy to change and play with the parameter values. If it was asked to improve the time complexity of the code, it is possible to get rid of the randomness. It could be done by:\n",
        "\n",
        "* An algorithm which assigns the maximum (or minimum) hours for d-1 days and\n",
        "assigning the sumHours - currentSum for the last day.\n",
        "* An algorithm which assigns the mean of minimum and maximum hours for d-1 days and assigning the sumHours - currentSum for the last day."
      ],
      "metadata": {
        "id": "CGKooJpPF90R"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "example",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}